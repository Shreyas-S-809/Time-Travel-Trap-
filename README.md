# Time-Travel-Trap

**How I Built a 99% Accurate Model That Was Completely Useless â€” and Replaced It with a Production-Ready System**

<p align="center">
  <img src="https://img.shields.io/badge/Machine%20Learning-Production%20Ready-brightgreen"/>
  <img src="https://img.shields.io/badge/Focus-Data%20Leakage-red"/>
  <img src="https://img.shields.io/badge/Model-XGBoost-blue"/>
  <img src="https://img.shields.io/badge/Deployment-Streamlit-orange"/>
  <a href="https://time-travel-trap.streamlit.app/"><img src="https://img.shields.io/badge/Live%20Demo-Streamlit-red"/></a>
</p>

<p align="center">
  <b>âš ï¸ High Accuracy â‰  Correct Model</b><br>
  
</p>

---

## Why This Project Exists

Most machine learning models fail in production, not because the algorithm is weak â€” but because the data pipeline is wrong.

This project demonstrates one of the most dangerous and common ML failures: **Data Leakage** â€” when a model accidentally learns from the future.

Instead of avoiding this mistake:

âœ… I intentionally created a leaky model  
ğŸ’¥ Proved why its accuracy was an illusion  
ğŸ› ï¸ Rebuilt a leakage-safe pipeline  
ğŸš€ Deployed a real forecasting system

---

## ğŸ¯ Project Objective

Build a sales forecasting system that exposes one of the most dangerous ML failures â€” Data Leakage â€” and shows how to fix it properly.

### What This Project Proves

1. Why random splits break time-series
2. Why future data contaminates models
3. Why evaluation can lie
4. How to design production-safe ML pipelines
5. How to deploy ML systems, not just notebooks

---

## ğŸ—ï¸ Architecture Overview

```
Raw Sales Data
      â†“
[ LEAKY PIPELINE ]
      â†“
ğŸ”¥ 99% Accuracy (Fake)
      â†“
âŒ Fails in Production
      â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â†“
[ ROBUST PIPELINE ]
      â†“
Lag + Rolling + Calendar
      â†“
TimeSeriesSplit CV
      â†“
âœ… Honest Performance
      â†“
ğŸš€ Streamlit Deployment
```

---

## ğŸ“ Repository Structure

```
project-6-time-travel-trap/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ sales_data.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_leaky_model_and_reality_check.ipynb
â”‚   â””â”€â”€ 02_robust_pipeline.ipynb
â”‚
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ xgboost_model.pkl
â”‚   â””â”€â”€ item_stats.csv
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Dataset Description

**Source:** Walmart Store Sales (Time-Series Retail Data)

**Link ğŸ‘‰**
[Click Here](https://www.kaggle.com/competitions/walmart-recruiting-store-sales-forecasting/data)

| Column      | Description        |
|-------------|-------------------|
| `Date`      | Time index        |
| `Store_ID`  | Store identifier  |
| `Item_ID`   | Product identifier|
| `Sales`     | Weekly sales      |
| `IsHoliday` | Holiday flag      |

Each `(Store_ID, Item_ID)` pair represents a real product evolving over time.

---

## ğŸš¨ Phase 1 â€” The Lie (Leaky Model)

### âŒ Intentional Mistakes (On Purpose)

#### 1ï¸âƒ£ Target Leakage
![alt text](image-1.png)

Used global statistics that include future data.

#### 2ï¸âƒ£ Temporal Leakage

Used random train-test split:

```python
train_test_split(shuffle=True)
```

This allows the model to learn from the future.

### ğŸ“ˆ Result (The Trap)

| Metric            | Value     |
|-------------------|-----------|
| RÂ² Score          | **~0.92** |
| Visual Fit        | Perfect   |
| Business Validity | âŒ        |

![alt text](image-2.png)

---

## ğŸ’¥ Phase 2 â€” Reality Check

Instead of forcing the score to drop artificially, this project proves a deeper truth:

**Once leakage exists, evaluation itself becomes meaningless.**

We cannot "fix" a leaky model â€” you must throw it away.

---

## ğŸ› ï¸ Phase 3 â€” The Fix (Robust Pipeline)

### âœ… Production Rules Enforced

âœ” No future data  
âœ” No global statistics  
âœ” No random splits  
âœ” Past â†’ Present â†’ Future only

### ğŸ§± Leakage-Safe Feature Engineering

| Feature            | Description                   | Safe |
|--------------------|-------------------------------|------|
| `Sales_Lag_7`      | Sales from last week          | âœ…   |
| `Sales_Rolling_30` | 30-day rolling mean (shifted) | âœ…   |
| `DayOfWeek`        | Calendar feature              | âœ…   |
| `Month`            | Seasonality                   | âœ…   |
| `IsWeekend`        | Weekend indicator             | âœ…   |

All rolling features are explicitly shifted to prevent future leakage.

### â±ï¸ Validation Strategy

```python
TimeSeriesSplit(n_splits=5)
```

**Why this matters:**

- Preserves temporal order
- Simulates real deployment
- Prevents silent leakage

### ğŸ“Š Honest Performance (Cross-Validation)

| Fold | RÂ²    |
|------|-------|
| 1    | ~0.72 |
| 2    | ~0.86 |
| 3    | ~0.89 |
| 4    | ~0.85 |
| 5    | ~0.93 |

**Interpretation:** Performance improves as more historical context becomes available.

---

## ğŸš€ Phase 4 â€” Deployment (Streamlit)

![alt text](image.png)

The final model is deployed as a **Scenario Planner**, not just a predictor.

### ğŸ”® App Capabilities

- Separate Store ID and Item ID selection
- Forecast future dates
- Apply marketing boost multiplier
- Display exact model input features
- Convert predictions to INR using live USDâ†’INR rates

### ğŸ§  Deployment Design Decisions

- Model and feature store saved separately
- Rolling statistics precomputed (feature-store pattern)
- Training and inference feature logic aligned
- Exchange rates cached safely

This avoids one of the most common ML failures: **trainingâ€“serving feature mismatch**.

### ğŸ§¾ Production Artifacts

- `xgboost_model.pkl` â†’ trained model
- `item_stats.csv` â†’ last known rolling statistics

The app never recomputes historical features â€” it consumes trusted context, just like a real system.

---

## Deployment Notes

**ğŸŒ Live Demo:** [Sales Forecaster on Streamlit](https://time-travel-trap.streamlit.app)

This project is deployed as a production-safe forecasting system, not just a trained model.

* The trained model (`xgboost_model.pkl`) and historical feature store (`item_stats.csv`) are saved separately to ensure trainingâ€“serving consistency.
* At inference time, the application recreates only leakage-safe features (lagged trends, rolling statistics, and calendar features).
* No future data or global statistics are accessed during prediction.
* The Streamlit app serves as a scenario planner, allowing controlled "what-if" adjustments without retraining the model.
* Live USD â†’ INR conversion is fetched safely and cached, with graceful fallback on failure.

---


## â–¶ï¸ Running the App Locally


1. **Clone the repository**
   ```bash
   git clone [LINK](https://github.com/Shreyas-S-809/Time-Travel-Trap-)
   cd Time-Travel-Trap
   ```

2. **Initialize and Activate Virtual Environment**
   ```powershell
   # Create environment
   python -m venv en

   # Activate (Windows)
   en\Scripts\activate
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the Application**
   ```bash
   streamlit run app.py
   ```

All heavy ML computation is already done â€” the app loads instantly.

---

## Project Status

âœ… End-to-End Complete  
âœ… Leakage-Safe  
âœ… Production-Ready

---

## ğŸ”® Possible Extensions

- Drift monitoring dashboard
- User feedback loop for retraining
- Feature store backed by a database
- Scheduled retraining pipelines

---

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**Thank You!**