# Time-Travel-Trap
How I Built a 99% Accurate Model That Was Completely Useless â€” and Replaced It with a Production-Ready System

<p align="center"> <img src="https://img.shields.io/badge/Machine%20Learning-Production%20Ready-brightgreen"/> <img src="https://img.shields.io/badge/Focus-Data%20Leakage-red"/> <img src="https://img.shields.io/badge/Model-XGBoost-blue"/> <img src="https://img.shields.io/badge/Deployment-Streamlit-orange"/> </p> <p align="center"> <b>âš ï¸ High Accuracy â‰  Correct Model</b><br> This project demonstrates why. </p>


ğŸš¨ Why This Project Exists

Most machine learning models fail in production, not because the algorithm is weak â€”
but because the data pipeline is wrong.

This project demonstrates one of the most dangerous and common ML failures:

Data Leakage â€” when a model accidentally learns from the future.
Instead of avoiding this mistake,

âœ… I intentionally created a leaky model
ğŸ’¥ Proved why its accuracy was an illusion
ğŸ› ï¸ Rebuilt a leakage-safe pipeline
ğŸš€ Deployed a real forecasting system


ğŸ¯ Project Objective

Build a sales forecasting system that exposes one of the most dangerous ML failures â€” Data Leakage â€” and shows how to fix it properly.

What This Project Proves : 
Why random splits break time-series
Why future data contaminates models
Why evaluation can lie
How to design production-safe ML pipelines
How to deploy ML systems, not just notebooks

## ğŸ—ï¸ Architecture Overview

```text
Raw Sales Data
      â†“
[ LEAKY PIPELINE ]
      â†“
ğŸ”¥ 99% Accuracy (Fake)
      â†“
âŒ Fails in Production
      â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â†“
[ ROBUST PIPELINE ]
      â†“
Lag + Rolling + Calendar
      â†“
TimeSeriesSplit CV
      â†“
âœ… Honest Performance
      â†“
ğŸš€ Streamlit Deployment

## ğŸ—ï¸ Repository Structure

```text
project-6-time-travel-trap/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ sales_data.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_leaky_model_and_reality_check.ipynb
â”‚   â””â”€â”€ 02_robust_pipeline.ipynb
â”‚
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ xgboost_model.pkl
â”‚   â””â”€â”€ item_stats.csv
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ“Š Dataset Description
Source: Walmart Store Sales (Time-Series Retail Data)

| Column      | Description        |
| ----------- | ------------------ |
| `Date`      | Time index         |
| `Store_ID`  | Store identifier   |
| `Item_ID`   | Product identifier |
| `Sales`     | Weekly sales       |
| `IsHoliday` | Holiday flag       |

Each (Store_ID, Item_ID) pair represents a real product evolving over time.

ğŸš¨ Phase 1 â€” The Lie (Leaky Model)
âŒ Intentional Mistakes (On Purpose)
1ï¸âƒ£ Target Leakage
![alt text](image-1.png)

Used global statistics that include future data.

2ï¸âƒ£ Temporal Leakage

Used random train-test split:

"train_test_split(shuffle=True)"


This allows the model to learn from the future.


ğŸ“ˆ Result (The Trap)

| Metric            | Value     |
| ----------------- | --------- |
| RÂ² Score          | **~0.92** |
| Visual Fit        | Perfect   |
| Business Validity | âŒ        |

![alt text](image-2.png)


ğŸ’¥ Phase 2 â€” Reality Check

Instead of forcing the score to drop artificially, this project proves a deeper truth:
Once leakage exists, evaluation itself becomes meaningless.
We cannot "fix" a leaky model â€” you must throw it away.

ğŸ› ï¸ Phase 3 â€” The  Fix (Robust Pipeline)
âœ… Production Rules Enforced

âœ” No future data
âœ” No global statistics
âœ” No random splits
âœ” Past â†’ Present â†’ Future only

ğŸ§± Leakage-Safe Feature Engineering

| Feature            | Description                   | Safe |
| ------------------ | ----------------------------- | ---- |
| `Sales_Lag_7`      | Sales from last week          | âœ…   |
| `Sales_Rolling_30` | 30-day rolling mean (shifted) | âœ…   |
| `DayOfWeek`        | Calendar feature              | âœ…   |
| `Month`            | Seasonality                   | âœ…   |
| `IsWeekend`        | Weekend indicator             | âœ…   |

All rolling features are explicitly shifted to prevent future leakage.


â±ï¸ Validation Strategy
TimeSeriesSplit(n_splits=5)


Why this matters:

Preserves temporal order
Simulates real deploymemt
Prevents silent leakage


ğŸ“Š Honest Performance (Cross - Validation)
| Fold | RÂ²    |
| ---- | ----- |
| 1    | ~0.72 |
| 2    | ~0.86 |
| 3    | ~0.89 |
| 4    | ~0.85 |
| 5    | ~0.93 |


Interpretation:
Performance improves as more historical context becomes available â€”
exactly what a real forecasting system should show.

ğŸš€ Phase 4 â€” Deployment (Streamlit)

![alt text](image.png)

The final model is deployed as a Scenario Planner, not just a predictor.

ğŸ”® App Capabilities

Separate Store ID and Item ID selection
Forecast future dates
Apply marketing boost multiplier
Display exact model input features
Convert predictions to INR using live USDâ†’INR rates

ğŸ§  Deployment Design Decisions

Model and feature store saved separately
Rolling statistics precomputed (feature-store pattern)
Training and inference feature logic aligned
Exchange rates cached safely
This avoids one of the most common ML failures:
trainingâ€“serving feature mismatch.

ğŸ§¾ Production Artifacts

xgboost_model.pkl â†’ trained model
item_stats.csv â†’ last known rolling statistics

The app never recomputes historical features â€” it consumes trusted context, just like a real system.

ğŸ“Œ Project Status

âœ… End-to-End Complete
âœ… Leakage-Safe
âœ… Production-Ready

ğŸ”® Possible Extensions

Drift monitoring dashboard
User feedback loop for retraining
Feature store backed by a database
Scheduled retraining pipelines.

ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.


Thank You..
